{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZu8HJDwzEttS1vAXVZkVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jedrzejwalega/RLHF-Gentle-Introduction/blob/main/RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What's RLHF and why the hype?**\n",
        "**ChatGPT** has taken the world by storm and for a good reason - it's an extremely powerful tool equipped not only in vast knowledge, but also eloquency that appeals to the human user. Large language models (LLMs) are not restricted to ChatGPT of course, but it's not an accident that OpenAI's tool is the most popular one. Quality wise it is easier to converse with than most other LLMs.\n",
        "\n",
        "Why is that? Well, one of the core reasons is a novel approach used in training ChatGPT called **Reinforced Learning from Human Feedback** or **RLHF** for short. As the name might suggest, it's a new trick for improving the model's conversational capabilities, involving humans in the training process.\n",
        "\n",
        "RLHF can be summarised as a way of guiding the model's learning by having humans directly rate its outputs, based on which the model adjusts itself to receive the biggest human rating possible. This helps its speech become more user friendly across multiple iterations.\n",
        "\n",
        "I'd like to dedicate this tutorial to introduce you to RLHF method, but before we dive deeper, let's cover some theoretical basis necessary to understand what's going on - supervised learning, reinforcement learning and how to create an LLM from scratch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xf5OM0cMWOQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Crash course through basics**\n",
        "##Supervised learning vs Reinforcement learning\n",
        "When we train machine learning models, in our case neural nets, we can do this through unsupervised, supervised and reinforced learning methods, the latter two being relevant for this tutorial.\n",
        "\n",
        "**Supervised learning** occurs when we have a dataset with ground truth labels. Our model learns to predict those labels. The difference between its predictions and ground truth is recorded as loss. We know an algorithm known as **gradient descent** that allows us to minimize this loss. Thus, across multiple iterations, it allows the net to adjust itself and become better at predicting the labels.\n",
        "\n",
        "**Reinforced learning** is a method that can be used when we don't have the luxury of a labeled dataset, but we still do know the end goal that we want to achieve. We allow our model a set of actions it can perform. Its goal is to learn a policy of using those actions to achieve the goal given to it. Just like in supervised learning the model tries multiple times. At each step it receives a **reward** which varies depending on how good the model is meeting its goal.\n",
        "\n",
        "Similarly to supervised learning we have an algorithm that aids us in updating the model into superior versions. In this case we're not minimizing the loss, but maximizing the reward. The popularly used algorithm is **Proximal Policy Optimization** (**PPO**). While I won't be discussing it here at large, its purpose is to not only update the model towards max reward, but also to keep the updates as modest as possible. Large updates are undesired, since they can lead to unstable training.\n",
        "\n",
        "The reinforcement learning method can be compared to placing a robot in a sandbox, allowing it a set of moves and letting it run freely by waving its arms and legs at leisure. The robot will regularly receive candy. The closer its hands are to the ground and the faster they move, the sweeter the candy will be. Eventually the robot will start digging to get the best candy possible."
      ],
      "metadata": {
        "id": "Z_B795O4blAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LLM from scratch**\n",
        "When hearing about training and LLM you might first think about the extremely resource intense process that painstakingly trains the model on gigantic amounts of data. While this is a good intuition, this can only be described as step one, as later on we have additional steps that help us forge the capable, but still rough model into a conversation partner.\n",
        "\n",
        "Steps:\n",
        "\n",
        "###1) **Pretraining**\n",
        "A supervised learning example. You train your model on previously mentioned unfiltered amounts of text. Since it's supervised learning the model will be predicting a label. It this case the model takes in a certain amount of text as input and tries to predict the next word (with the word being the label). During this phase the LLM learns to understand human speech and fundamental concepts. It's also absolutely the most resource heavy step.\n",
        "\n",
        "**Model output example in response to a prompt (in cursive):**\n",
        "\n",
        "The best movie of all time *coming soon on DVD.*\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###2) **Supervised finetuning**\n",
        "Another supervised learning step. The pretrained model has learned to do its job - to predict the next word in a sentance. That's not quite what we're after. Thus, we need to use a technique known as finetuning to further train the model to our liking. This time a more modest dataset is enough for the already pretrained model, since we just need to adjust it a little bit.\n",
        "\n",
        "Since the required dataset sizes are now more manageable, we use this to provide higher quality, filtered data. Our new dataset should represent conversation examples, as well as non-biased and non-opinionated data to prune unwanted behavior from the model.\n",
        "\n",
        "In practice the finetuning happens by freezing (preventing from change) most of the model's weights except for the last few layers. Only those are trained, which adjusts the model to act in a more preferrable way.\n",
        "\n",
        "**Model output example in response to a prompt:**\n",
        "\n",
        "The best movie of all time\n",
        "\n",
        "*Released in 2009, \"Avatar\" by James Cameron can be regarded as the best movie of all time due to it grossing $2,923,706,026, the highest amount out of all movies in history.*\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###3) **RLHF**\n",
        "We already have a capable, conversational model. But some of its outputs are kind of dry, maybe not what the consumers would expect. Maybe sometimes the model outputs too little information and sometimes too much? This is adjusted through RLHF (expanded upon in later section) by incorporating human preferances into the model behavior. RLHF is a reinforcement learning step.\n",
        "\n",
        "**Model output example in response to a prompt:**\n",
        "\n",
        "The best movie of all time\n",
        "\n",
        "*Certainly! The term \"best movie of all time\" carries an intrinsic bias that makes it a purely subjective choice, varying from person to person. However, if we're looking for a quantifiable score to answer your question, we could use box office returns. As of my latest knowledge update of 29th November 2023, the highest grossing film was James Cameron's \"Avatar\", released in 2009. It's box office return was an impressive sum of $2,923,706,026, potentially earning it the spot as \"the best movie of all time\", at least in terms of box office returns.*\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Nowadays it is standard practice in creating an LLM to skip either of the above steps. The trained models are readily available on the internet, so it's more convenient and affordable to grab an already existing model and finetune it to your use case, either through supervised finetuning, RLHF or both.\n",
        "\n"
      ],
      "metadata": {
        "id": "TcmP0VDFFNd5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOftF_uKOdeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}