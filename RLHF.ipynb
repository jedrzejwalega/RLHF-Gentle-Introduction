{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEHJ+OOFn8PXi06so8W6mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jedrzejwalega/RLHF-Gentle-Introduction/blob/main/RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What's RLHF and why the hype?**\n",
        "**ChatGPT** has taken the world by storm and for a good reason - it's an extremely powerful tool equipped not only in vast knowledge, but also eloquency that appeals to the human user. Large language models (LLMs) are not restricted to ChatGPT of course, but it's not an accident that OpenAI's tool is the most popular one. Quality wise it is easier to converse with than most other LLMs.\n",
        "\n",
        "Why is that? Well, one of the core reasons is a novel approach used in training ChatGPT called **Reinforced Learning from Human Feedback** or **RLHF** for short. As the name might suggest, it's a new trick for improving the model's conversational capabilities, involving humans in the training process.\n",
        "\n",
        "RLHF can be summarised as a way of guiding the model's learning by having humans directly rate its outputs, based on which the model adjusts itself to receive the biggest human rating possible. This helps its speech become more user friendly across multiple iterations.\n",
        "\n",
        "I'd like to dedicate this tutorial to introduce you to RLHF method, but before we dive deeper, let's cover some theoretical basis necessary to understand what's going on - supervised learning, reinforcement learning and how to create an LLM from scratch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xf5OM0cMWOQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Crash course through basics**\n",
        "##Supervised learning vs Reinforcement learning\n",
        "When we train machine learning models, in our case neural nets, we can do this through unsupervised, supervised and reinforced learning methods, the latter two being relevant for this tutorial.\n",
        "\n",
        "**Supervised learning** occurs when we have a dataset with ground truth labels. Our model learns to predict those labels. The difference between its predictions and ground truth is recorded as loss. We know an algorithm known as **gradient descent** that allows us to minimize this loss. Thus, across multiple iterations, it allows the net to adjust itself and become better at predicting the labels.\n",
        "\n",
        "**Reinforced learning** is a method that can be used when we don't have the luxury of a labeled dataset, but we still do know the end goal that we want to achieve. We allow our model a set of actions it can perform. Its goal is to learn a policy of using those actions to achieve the goal given to it. Just like in supervised learning the model tries multiple times. At each step it receives a **reward** which varies depending on how good the model is meeting its goal.\n",
        "\n",
        "Similarly to supervised learning we have an algorithm that aids us in updating the model into superior versions. In this case we're not minimizing the loss, but maximizing the reward. The popularly used algorithm is **Proximal Policy Optimization** (**PPO**). While I won't be discussing it here at large, its purpose is to not only update the model towards max reward, but also to keep the updates as modest as possible. Large updates are undesired, since they can lead to unstable training.\n",
        "\n",
        "The reinforcement learning method can be compared to placing a robot in a sandbox, allowing it a set of moves and letting it run freely by waving its arms and legs at leisure. The robot will regularly receive candy. The closer its hands are to the ground and the faster they move, the sweeter the candy will be. Eventually the robot will start digging to get the best candy possible."
      ],
      "metadata": {
        "id": "Z_B795O4blAY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsCLm-UChZoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}